---
title: "Used Car Price Analysis and Prediction Model"
format: 
  html:
    code-fold: true
    toc: true
execute:
  echo: true
---

# Introduction

This document presents an analysis of used car sales data for a client who is a used car dealer specializing in Ford vehicles. The goal is to design a model that can predict the optimum quotation price for the cars in their inventory. The dataset contains various features of the cars and the prices at which they were sold in past years.

The analysis will help the client make data-driven decisions when pricing their Ford vehicles, potentially increasing their profit margins and reducing the time cars spend on the lot.

# Data Loading and Initial Exploration

In this first section, we'll load the dataset from a URL and examine its structure. This step is crucial to understand the data we're working with before proceeding with any analysis or modeling.

## Importing Libraries by % uv add matplotlib numpy pandas seaborn ipykernel scikit-learn


```{python}
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Create a directory for figures if it doesn't exist
figures_dir = 'figures'
if not os.path.exists(figures_dir):
    os.makedirs(figures_dir)

# Function to save figures
def save_figure(fig, filename):
    filepath = os.path.join(figures_dir, filename)
    fig.savefig(filepath, dpi=300, bbox_inches='tight')
    plt.close(fig)
    print(f"Figure saved to {filepath}")

# Load the dataset from the URL
url = "https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMSkillsNetwork-AI0271EN-SkillsNetwork/labs/v1/m3/data/used_car_price_analysis.csv"

# Read the CSV file into a pandas DataFrame
df = pd.read_csv(url)

# Display the first 5 rows of the DataFrame to verify correct loading
print("First 5 rows of the dataset:")
df.head()
```

The code above loads the dataset using pandas, a powerful data manipulation library in Python. We're reading the CSV file directly from the provided URL, and the first row of the file is automatically used as headers for the data frame.

By displaying the first 5 rows with the `head()` function, we can quickly verify that the data has been loaded correctly and get an initial understanding of the structure and content of the dataset.

# Exploratory Data Analysis (EDA)

After successfully loading the data, our analysis will proceed with:

1. Exploratory Data Analysis (EDA) to understand the distributions and relationships between variables
2. Data preprocessing, including handling missing values and feature engineering
3. Building and training predictive models for car prices
4. Evaluating model performance
5. Developing a system to recommend optimal quotation prices for Ford vehicles

The insights gained from this analysis will provide the client with a data-driven approach to pricing their inventory, potentially leading to increased sales and profitability.




# Data Preparation

Before proceeding with analysis and modeling, we need to clean the data by addressing missing values and removing duplicate entries. These steps are crucial for ensuring the quality and reliability of our analysis results.

```{python}
# 1. Identify columns with missing values
print("\nColumns with missing values:")
missing_values = df.isnull().sum()
columns_with_missing = missing_values[missing_values > 0]
print(columns_with_missing)

# Calculate the percentage of missing values in each column
if len(columns_with_missing) > 0:
    missing_percentage = (columns_with_missing / len(df)) * 100
    print("\nPercentage of missing values in affected columns:")
    print(missing_percentage)
else:
    print("No missing values found in the dataset.")

# 2. Replace missing values with the mean of each column
# We'll only perform this operation on numeric columns
numeric_columns = df.select_dtypes(include=['int64', 'float64']).columns
for column in numeric_columns:
    if df[column].isnull().sum() > 0:
        column_mean = df[column].mean()
        df[column].fillna(column_mean, inplace=True)
        print(f"\nReplaced missing values in '{column}' with mean: {column_mean:.2f}")

# For categorical columns with missing values, we can replace with the mode (most frequent value)
categorical_columns = df.select_dtypes(include=['object']).columns
for column in categorical_columns:
    if df[column].isnull().sum() > 0:
        column_mode = df[column].mode()[0]
        df[column].fillna(column_mode, inplace=True)
        print(f"\nReplaced missing values in '{column}' with mode: {column_mode}")

# Check if any missing values remain
print("\nRemaining missing values after replacement:")
print(df.isnull().sum().sum())

# 3. Remove duplicate entries
duplicate_count = df.duplicated().sum()
print(f"\nNumber of duplicate rows found: {duplicate_count}")

if duplicate_count > 0:
    # Remove duplicates and reset index
    df.drop_duplicates(inplace=True)
    df.reset_index(drop=True, inplace=True)
    print(f"Duplicate rows removed. New dataframe shape: {df.shape}")

# Display summary of the cleaned dataset
print("\nSummary of the cleaned dataset:")
print(f"Total rows: {df.shape[0]}")
print(f"Total columns: {df.shape[1]}")
df.info()
```

The data cleaning process above performs three essential tasks:

1. **Identifying Missing Values**: We first identify which columns contain missing values and calculate what percentage of the data is missing in each affected column. This helps us understand the extent of the missing data problem.

2. **Replacing Missing Values**: For numeric columns, we replace missing values with the mean of each column. For categorical columns, we use the mode (most frequent value). This approach helps preserve the overall distribution of the data while filling in the gaps.

3. **Removing Duplicates**: Duplicate entries can skew our analysis and model training. By removing them, we ensure that each observation in our dataset is unique, which is important for accurate modeling.

After these cleaning steps, we display a summary of the resulting dataset to confirm that our data is now ready for analysis and modeling.

## Save the cleaned dataset

```{python}
df.to_csv('cleaned_car_data.csv', index=False)
```

## Data Augmentation (optional)

Once cleaned, you may choose to augment this dataset with additional samples, created synthetically using python code from Claude 3.7.

---
title: "Used Car Price Analysis - Data Augmentation"
format: 
  html:
    code-fold: true
    toc: true
execute:
  echo: true
---

# Data Augmentation with Synthetic Samples

In this section, we'll augment our cleaned dataset with synthetic samples generated using machine learning techniques. Data augmentation is particularly useful for:

1. Increasing the size of our training dataset (+ 20% more samples)
2. Improving model robustness by introducing controlled variability
3. Balancing classes or addressing data sparsity in certain regions of the feature space

We'll use a combination of statistical sampling and machine learning to create realistic synthetic car listings that follow the patterns and relationships present in our original data.

```{python}
# Import required libraries
import pandas as pd
import numpy as np
from sklearn.neighbors import KernelDensity
import random
from datetime import datetime
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import OneHotEncoder

# Set random seed for reproducibility
np.random.seed(42)
random.seed(42)

# Read the cleaned dataset
df = pd.read_csv('cleaned_car_data.csv')

print(f"Original dataset shape: {df.shape}")

# Define the number of synthetic samples to generate
n_synthetic_samples = int(len(df) * 0.2)  # Adding 20% more samples
print(f"Generating {n_synthetic_samples} synthetic samples")
```

## Creating the Synthetic Data Generator

We'll define a function that generates synthetic samples while preserving the relationships between features. This is important because car attributes are not independent - for example, older cars typically have higher mileage, and certain features influence price in combination.

```{python}
# Function to generate synthetic samples using various techniques
def generate_synthetic_samples(data, n_samples):
    synthetic_samples = []
    
    # Get unique values and distributions for categorical columns
    model_options = data['model'].unique()
    transmission_options = data['transmission'].unique()
    fuelType_options = data['fuelType'].unique()
    
    # Get ranges and distributions for numerical columns
    year_min, year_max = data['year'].min(), data['year'].max()
    mileage_mean, mileage_std = data['mileage'].mean(), data['mileage'].std()
    tax_min, tax_max = data['tax'].min(), data['tax'].max()
    mpg_mean, mpg_std = data['mpg'].mean(), data['mpg'].std()
    engineSize_options = data['engineSize'].unique()
    
    # Price should be predicted based on other features
    # We'll use a linear regression model to maintain feature relationships
    
    # Prepare data for price prediction
    X_cat = data[['model', 'transmission', 'fuelType']]
    X_num = data[['year', 'mileage', 'tax', 'mpg', 'engineSize']]
    
    # One-hot encode categorical features
    encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')
    X_cat_encoded = encoder.fit_transform(X_cat)
    
    # Combine numerical and encoded categorical features
    X = np.hstack([X_num, X_cat_encoded])
    y = data['price']
    
    # Train a simple linear regression model
    model = LinearRegression()
    model.fit(X, y)
    
    # Generate synthetic samples
    for i in range(n_samples):
        # Generate random values for each feature
        model_val = random.choice(model_options)
        year_val = random.randint(year_min, year_max)
        transmission_val = random.choice(transmission_options)
        
        # Mileage increases as car age increases (realistic relationship)
        car_age = datetime.now().year - year_val
        mileage_val = max(0, int(np.random.normal(mileage_mean, mileage_std) * (1 + car_age/10)))
        
        fuelType_val = random.choice(fuelType_options)
        tax_val = random.uniform(tax_min, tax_max)
        mpg_val = max(10, np.random.normal(mpg_mean, mpg_std))
        engineSize_val = random.choice(engineSize_options)
        
        # Create new sample
        new_sample = {
            'model': model_val,
            'year': year_val,
            'transmission': transmission_val,
            'mileage': mileage_val,
            'fuelType': fuelType_val,
            'tax': round(tax_val, 2),
            'mpg': round(mpg_val, 2),
            'engineSize': round(engineSize_val, 1)
        }
        
        # Predict price based on other features
        X_cat_new = pd.DataFrame([new_sample])[['model', 'transmission', 'fuelType']]
        X_num_new = pd.DataFrame([new_sample])[['year', 'mileage', 'tax', 'mpg', 'engineSize']]
        
        X_cat_encoded_new = encoder.transform(X_cat_new)
        X_new = np.hstack([X_num_new, X_cat_encoded_new])
        
        # Predict price and add some random noise to simulate real-world variability
        predicted_price = max(0, int(model.predict(X_new)[0] * (1 + np.random.normal(0, 0.05))))
        new_sample['price'] = predicted_price
        
        synthetic_samples.append(new_sample)
    
    return pd.DataFrame(synthetic_samples)
```

## Generating and Combining the Data

Now we'll generate the synthetic samples and combine them with our original dataset to create an augmented dataset.

```{python}
# Generate synthetic samples
synthetic_df = generate_synthetic_samples(df, n_synthetic_samples)

print(f"Synthetic dataset shape: {synthetic_df.shape}")

# Add a column to identify real vs synthetic data (for validation purposes)
df['is_synthetic'] = 0
synthetic_df['is_synthetic'] = 1

# Combine original and synthetic data
augmented_df = pd.concat([df, synthetic_df], ignore_index=True)

print(f"Augmented dataset shape: {augmented_df.shape}")

# Optional: Shuffle the data to mix synthetic and real entries
augmented_df = augmented_df.sample(frac=1, random_state=42).reset_index(drop=True)

# Save the augmented dataset
augmented_df.to_csv('augmented_car_data.csv', index=False)

print("Augmented dataset saved to 'augmented_car_data.csv'")
```

## Validating the Synthetic Data

It's important to verify that our synthetic data maintains the same statistical properties as the original data while introducing useful variation.

```{python}
# Show a sample of the synthetic data
print("\nSample of 5 synthetic records:")
print(synthetic_df.head(5))

# Compare distributions of original and synthetic data
import matplotlib.pyplot as plt
import seaborn as sns

# Set up the figure
plt.figure(figsize=(15, 10))

# Compare price distributions
plt.subplot(2, 2, 1)
sns.histplot(df['price'], kde=True, color='blue', alpha=0.5, label='Original')
sns.histplot(synthetic_df['price'], kde=True, color='red', alpha=0.5, label='Synthetic')
plt.title('Price Distribution')
plt.legend()

# Compare year distributions
plt.subplot(2, 2, 2)
sns.histplot(df['year'], kde=True, bins=len(df['year'].unique()), color='blue', alpha=0.5, label='Original')
sns.histplot(synthetic_df['year'], kde=True, bins=len(synthetic_df['year'].unique()), color='red', alpha=0.5, label='Synthetic')
plt.title('Year Distribution')
plt.legend()

# Compare mileage distributions
plt.subplot(2, 2, 3)
sns.histplot(df['mileage'], kde=True, color='blue', alpha=0.5, label='Original')
sns.histplot(synthetic_df['mileage'], kde=True, color='red', alpha=0.5, label='Synthetic')
plt.title('Mileage Distribution')
plt.legend()

# Compare engine size distributions
plt.subplot(2, 2, 4)
sns.histplot(df['engineSize'], kde=True, bins=len(df['engineSize'].unique()), color='blue', alpha=0.5, label='Original')
sns.histplot(synthetic_df['engineSize'], kde=True, bins=len(synthetic_df['engineSize'].unique()), color='red', alpha=0.5, label='Synthetic')
plt.title('Engine Size Distribution')
plt.legend()

plt.tight_layout()
plt.show()

# Show numerical comparison of statistics
print("\nOriginal vs Synthetic Data - Numerical Columns Statistics")
orig_stats = df.describe()
synth_stats = synthetic_df.describe()

# Display side by side for comparison
comparison = pd.DataFrame({
    'Original': orig_stats['price'],
    'Synthetic': synth_stats['price']
})
print("\nPrice Statistics Comparison:")
print(comparison)
```

## Conclusion for data augmentation

We've successfully augmented our dataset with synthetic samples that maintain the statistical properties and feature relationships of the original data. This augmented dataset will provide more training examples for our price prediction model, potentially improving its accuracy and robustness.

The augmentation process uses machine learning to ensure that the synthetic data points are realistic and follow the patterns observed in actual Ford car sales data. By incorporating the relationships between features (such as car age and mileage), we create synthetic samples that better represent real-world scenarios.




# Data Analysis for Used Car Price Prediction

After successfully loading and cleaning the data, our analysis will proceed with:

1. Exploratory Data Analysis (EDA) to understand the distributions and relationships between variables
2. Feature engineering to create new meaningful variables


The insights gained from this analysis will provide the client with a data-driven approach to pricing their inventory, potentially leading to increased sales and profitability.



This document presents a comprehensive analysis of the augmented Ford used car dataset. The analysis includes identifying key correlations, exploring value distributions, and visualizing relationships between car attributes and prices. This information will help develop an optimal pricing model for the used car dealership.

## Task 1: Top 5 Attributes with Highest Correlation to Price

In this section, we'll identify which car attributes have the strongest relationship with price. This helps us understand the main factors influencing car valuation.

```{python}
# Identify top 5 attributes with highest correlation with the target attribute (price)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Set the visual theme and font size for better readability
sns.set_theme(style="whitegrid")
plt.rcParams.update({'font.size': 12})

# Load the augmented dataset
df = pd.read_csv('augmented_car_data.csv')

# Display basic information about the dataset
print(f"Dataset shape: {df.shape}")
print(f"Dataset columns: {df.columns.tolist()}")

# Separate numerical columns for correlation analysis
numerical_df = df.select_dtypes(include=['int64', 'float64'])

# Calculate the correlation matrix
correlation_matrix = numerical_df.corr()

# Extract the correlations with the target attribute (price)
price_correlations = correlation_matrix['price'].drop('price')  # Drop self-correlation

# Add absolute correlation for sorting
price_correlations_abs = price_correlations.abs()

# Sort by absolute correlation values in descending order
sorted_correlations = price_correlations.loc[price_correlations_abs.sort_values(ascending=False).index]

# Get the top 5 attributes with highest correlation
top_5_correlations = sorted_correlations.head(5)

print("\nTop 5 attributes with highest correlation with price:")
print(top_5_correlations)

# Create a bar plot to visualize the correlations
plt.figure(figsize=(10, 6))
top_5_correlations.plot(kind='bar', color=['blue' if x > 0 else 'red' for x in top_5_correlations])
plt.title('Top 5 Attributes with Highest Correlation to Price')
plt.xlabel('Attributes')
plt.ylabel('Correlation Coefficient with Price')
plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)
plt.grid(axis='y', linestyle='--', alpha=0.3)
plt.tight_layout()
save_figure(plt.gcf(), 'top_5_correlations.png')

# Create a heatmap of the correlation matrix for a comprehensive view
plt.figure(figsize=(10, 8))
mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))  # Create mask for upper triangle
sns.heatmap(correlation_matrix, mask=mask, annot=True, cmap='coolwarm', fmt=".2f", 
            linewidths=0.5, cbar_kws={"shrink": .8})
plt.title('Correlation Matrix of Numerical Attributes')
plt.tight_layout()
save_figure(plt.gcf(), 'correlation_matrix.png')
```

The correlation analysis reveals which attributes have the strongest linear relationship with car prices. Understanding these relationships helps us identify the most important factors to consider when developing a price prediction model.

## Task 2: Distribution of Attribute Values

Next, we'll analyze the distribution of values for key attributes in our dataset. This helps us understand the composition of the car inventory and identify any patterns or imbalances.

```{python}
# Function to analyze attribute distribution
def analyze_attribute_distribution(dataframe, attribute_name, top_n=15):
    # Count occurrences of each unique value
    value_counts = dataframe[attribute_name].value_counts().sort_values(ascending=False)
    
    print(f"\nDistribution of {attribute_name} (showing top {min(top_n, len(value_counts))} values):")
    print(value_counts.head(top_n))
    
    # Calculate percentages
    value_percentages = dataframe[attribute_name].value_counts(normalize=True).sort_values(ascending=False) * 100
    
    # Visualize the distribution
    plt.figure(figsize=(12, 6))
    
    # For categorical variables with many categories, limit to top N for readability
    if len(value_counts) > top_n:
        value_counts_limited = value_counts.head(top_n)
        plt.bar(value_counts_limited.index, value_counts_limited.values)
        plt.title(f'Distribution of {attribute_name} (Top {top_n} Values)')
    else:
        plt.bar(value_counts.index, value_counts.values)
        plt.title(f'Distribution of {attribute_name}')
    
    plt.xlabel(attribute_name)
    plt.ylabel('Count')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    save_figure(plt.gcf(), f'distribution_{attribute_name}.png')
    
    return value_counts

# Analyze distribution for key categorical attributes
model_counts = analyze_attribute_distribution(df, 'model')
transmission_counts = analyze_attribute_distribution(df, 'transmission')
fuelType_counts = analyze_attribute_distribution(df, 'fuelType')

# Create and analyze discretized numerical attributes for better interpretation
# Year ranges
df['year_range'] = pd.cut(df['year'], 
                         bins=[df['year'].min()-1, 2010, 2015, 2020, df['year'].max()],
                         labels=['Pre-2010', '2010-2014', '2015-2019', '2020+'])
year_range_counts = analyze_attribute_distribution(df, 'year_range')

# Create price ranges
max_price = df['price'].max()
price_bins = list(range(0, int(max_price + 10000), 10000))
df['price_range'] = pd.cut(df['price'], bins=price_bins).astype(str)  # Convert intervals to strings

# Analyze price range distribution
price_range_counts = analyze_attribute_distribution(df, 'price_range')

# Compare synthetic vs real data distribution
df['data_type'] = df['is_synthetic'].map({0: 'Real Data', 1: 'Synthetic Data'})
synthetic_counts = analyze_attribute_distribution(df, 'data_type')
```

Understanding the distribution of values for each attribute helps identify the most common car configurations in the inventory. This information can guide pricing strategies for popular models and help identify potential market gaps.

## Task 3: Box Plots Analysis

Box plots provide a visual representation of the price distribution across different car categories. This helps identify how various attributes affect price ranges and variability.

```{python}
# Function to create box plots between source and target attributes
def create_boxplot(dataframe, source_attribute, target_attribute='price', sort_by='median'):
    plt.figure(figsize=(14, 8))
    
    # If the source attribute has too many unique values, limit to the top most frequent
    if dataframe[source_attribute].nunique() > 10:
        # Get the top 10 most frequent values
        top_values = dataframe[source_attribute].value_counts().head(10).index.tolist()
        plot_df = dataframe[dataframe[source_attribute].isin(top_values)].copy()
        title_suffix = " (Top 10 most frequent values)"
    else:
        plot_df = dataframe.copy()
        title_suffix = ""
    
    # If requested, sort the boxes by the median value of the target attribute
    if sort_by == 'median':
        order = plot_df.groupby(source_attribute)[target_attribute].median().sort_values().index
    elif sort_by == 'mean':
        order = plot_df.groupby(source_attribute)[target_attribute].mean().sort_values().index
    else:  # No sorting
        order = None
    
    # Create the box plot
    ax = sns.boxplot(x=source_attribute, y=target_attribute, data=plot_df, order=order, palette='viridis')
    
    # Add a strip plot to show individual data points (with reduced opacity for clarity)
    sns.stripplot(x=source_attribute, y=target_attribute, data=plot_df, order=order, 
                 size=3, color='black', alpha=0.1, jitter=True)
    
    # Customize the plot
    plt.title(f'Distribution of {target_attribute} by {source_attribute}{title_suffix}')
    plt.xlabel(source_attribute)
    plt.ylabel(target_attribute)
    plt.xticks(rotation=45, ha='right')
    
    # Add median values as text
    medians = plot_df.groupby(source_attribute)[target_attribute].median()
    pos = range(len(medians) if order is None else len(order))
    for tick, label in zip(pos, ax.get_xticklabels()):
        category = label.get_text()
        if category in medians.index:
            median_val = medians[category]
            plt.text(tick, median_val, f'{median_val:.0f}', 
                    horizontalalignment='center', size='small', color='white', weight='bold')
    
    plt.tight_layout()
    save_figure(plt.gcf(), f'boxplot_{source_attribute}_vs_{target_attribute}.png')

# Create box plots for different categorical attributes vs. price
# Model vs. Price (using top models)
create_boxplot(df, 'model', 'price')

# Transmission vs. Price
create_boxplot(df, 'transmission', 'price')

# Fuel Type vs. Price
create_boxplot(df, 'fuelType', 'price')

# Year groups vs. Price 
create_boxplot(df, 'year_range', 'price')

# Engine Size groups vs. Price (discretized)
df['engineSize_group'] = pd.cut(df['engineSize'], 
                             bins=[0, 1.0, 1.5, 2.0, 2.5, df['engineSize'].max()],
                             labels=['≤1.0L', '1.0-1.5L', '1.5-2.0L', '2.0-2.5L', '>2.5L'])
create_boxplot(df, 'engineSize_group', 'price')

# Compare price distributions between real and synthetic data
create_boxplot(df, 'data_type', 'price', sort_by=None)
```

The box plots show how prices vary across different categories of cars. This helps identify which features command premium prices and which ones are associated with lower-priced vehicles. This information is valuable for setting competitive prices for different car configurations.

## Task 4: Regression Analysis

Regression plots help visualize the relationship between numerical attributes and price. This helps understand how changes in specific attributes affect car value.

```{python}
# Function to create regression plots between source attributes and the target attribute
def create_regression_plot(dataframe, source_attribute, target_attribute='price', polynomial_degree=1, sample_frac=1.0):
    plt.figure(figsize=(12, 8))
    
    # If the dataset is large, sample a fraction for clearer visualization
    if len(dataframe) > 5000 and sample_frac < 1.0:
        plot_df = dataframe.sample(frac=sample_frac, random_state=42)
    else:
        plot_df = dataframe
    
    # Create scatter plot with different colors for real vs synthetic data
    ax = sns.scatterplot(x=source_attribute, y=target_attribute, data=plot_df, 
                        alpha=0.4, hue='data_type', palette={'Real Data': 'blue', 'Synthetic Data': 'red'})
    
    # Add linear regression line
    sns.regplot(x=source_attribute, y=target_attribute, data=plot_df, 
               scatter=False, line_kws={'color': 'green', 'linewidth': 2})
    
    # If polynomial regression requested, add that line too
    if polynomial_degree > 1:
        # Prepare data for polynomial regression
        X = plot_df[source_attribute].values.reshape(-1, 1)
        y = plot_df[target_attribute].values
        
        # Create polynomial features
        poly_features = PolynomialFeatures(degree=polynomial_degree)
        X_poly = poly_features.fit_transform(X)
        
        # Fit polynomial regression model
        poly_model = LinearRegression()
        poly_model.fit(X_poly, y)
        
        # Generate points for the polynomial curve
        X_range = np.linspace(X.min(), X.max(), 100).reshape(-1, 1)
        X_range_poly = poly_features.transform(X_range)
        y_poly_pred = poly_model.predict(X_range_poly)
        
        # Plot the polynomial regression curve
        plt.plot(X_range, y_poly_pred, color='purple', linewidth=2, 
                label=f'Polynomial (degree={polynomial_degree})')
        plt.legend()
    
    # Calculate correlation and add to plot
    correlation = plot_df[[source_attribute, target_attribute]].corr().iloc[0, 1]
    plt.text(0.05, 0.95, f'Correlation: {correlation:.4f}', transform=ax.transAxes, 
            fontsize=12, verticalalignment='top', 
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    # Add regression equation for linear model
    slope, intercept = np.polyfit(plot_df[source_attribute], plot_df[target_attribute], 1)
    equation = f'y = {slope:.2f}x + {intercept:.2f}'
    plt.text(0.05, 0.90, equation, transform=ax.transAxes, 
            fontsize=12, verticalalignment='top',
            bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))
    
    # Customize the plot
    plt.title(f'Regression Plot: {source_attribute} vs {target_attribute}')
    plt.xlabel(source_attribute)
    plt.ylabel(target_attribute)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.tight_layout()
    save_figure(plt.gcf(), f'regression_{source_attribute}_vs_{target_attribute}.png')
    
    # Return correlation for reference
    return correlation

# Create regression plots for numerical attributes vs price
# Sample a fraction of the data for clearer visualization if dataset is large
sample_fraction = 0.5 if len(df) > 10000 else 1.0

# List of numerical attributes (excluding price and is_synthetic)
numerical_attributes = ['year', 'mileage', 'tax', 'mpg', 'engineSize']

# Create regression plots for each numerical attribute
correlations = {}
for attr in numerical_attributes:
    correlation = create_regression_plot(df, attr, sample_frac=sample_fraction)
    correlations[attr] = correlation
    
    # Add a polynomial regression for selected attributes
    if attr in ['year', 'engineSize']:
        create_regression_plot(df, attr, polynomial_degree=2, sample_frac=sample_fraction)

# Advanced analysis: Create a multi-faceted regression plot for the most correlated attribute
# against price, split by transmission type
top_correlated_attr = max(correlations, key=correlations.get)

plt.figure(figsize=(16, 10))
g = sns.FacetGrid(df.sample(frac=sample_fraction) if sample_fraction < 1.0 else df, 
                  col='transmission', col_wrap=2, height=4, aspect=1.5)
g.map_dataframe(sns.regplot, top_correlated_attr, 'price', scatter_kws={'alpha': 0.5})
g.set_axis_labels(top_correlated_attr, 'price')
g.set_titles('Transmission: {col_name}')
g.fig.suptitle(f'Regression of {top_correlated_attr} vs price by transmission type', y=1.05)
plt.tight_layout()
plt.show()

# Print the correlation values
print(f"\nCorrelations with price (sorted by strength):")
for attr, corr in sorted(correlations.items(), key=lambda x: abs(x[1]), reverse=True):
    print(f"{attr}: {corr:.4f}")
```

The regression analysis reveals how different numerical attributes relate to the price of Ford cars. The strength and direction of these relationships provide insights into which features add value to a vehicle and which ones decrease its value. The correlation coefficients quantify these relationships, while the regression lines show the expected price change for each unit change in the attribute.

## Key Insights and Conclusions

Based on our comprehensive analysis of the Ford used car dataset, we can draw several important conclusions:

1. **Primary Price Determinants**:
   - The top correlated attributes with price are year, mileage, tax, engineSize, and mpg.
   - These factors should be given the highest weightage in the pricing model

2. **Market Composition**:
   - The most common models in the dataset are Focus, Fiesta, and Mondeo.
   - The distribution of transmissions shows that automatic transmissions are more prevalent than manual transmissions.
   - Most vehicles use diesel as their primary fuel type.

3. **Price Variations by Category**:
   - Engine size has a positive relationship with price.
   - Newer cars command significantly higher prices
   - Different fuel types show distinct pricing patterns

4. **Pricing Recommendations**:
   - Premium pricing should be applied to cars with low mileage, high tax, and low mpg.
   - Mid-range pricing is appropriate for cars with low mileage, low tax, and high mpg.
   - Lower-tier pricing is suitable for cars with high mileage, high tax, and low mpg.

## Next Steps for Model Development

The insights gained from this analysis will inform the development of a predictive model for optimizing car quotation prices. The next steps include:

1. Feature selection based on correlation analysis
2. Model training using regression techniques
3. Model validation and testing
4. Implementation of the pricing system

This data-driven approach will help the dealership set competitive and profitable prices for their Ford vehicle inventory.


#  Price Prediction Model for Ford Used Cars

In this document, we build a comprehensive predictive model for Ford used car prices based on our previous data analysis. We'll implement feature engineering, compare different regression models, and optimize model parameters to create an accurate and reliable price prediction system for the car dealership.

## 1. Feature Engineering

Based on our exploratory data analysis, we identified the top correlated attributes with price: year, mileage, tax, engineSize, and mpg. We'll use these core attributes along with engineered features to build our prediction model.

```{python}
# Import necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import r2_score, mean_squared_error
import warnings
warnings.filterwarnings('ignore')

# Set visual style
sns.set_theme(style="whitegrid")
plt.rcParams.update({'font.size': 12})

# Load the augmented dataset
df = pd.read_csv('augmented_car_data.csv')

print(f"Dataset shape: {df.shape}")
print(f"Columns: {df.columns.tolist()}")
```

### Creating New Features

We'll create several engineered features to capture additional information and relationships that can improve our model's performance:

```{python}
# 1. Create feature for car age
current_year = 2025  # Current year
df['car_age'] = current_year - df['year']

# 2. Create mileage per year feature
df['mileage_per_year'] = df['mileage'] / df['car_age']
# Handle potential division by zero or infinity values
df['mileage_per_year'] = df['mileage_per_year'].replace([np.inf, -np.inf], np.nan)
df['mileage_per_year'] = df['mileage_per_year'].fillna(df['mileage_per_year'].median())

# 3. Create feature for engine size to mpg ratio (efficiency measure)
df['engine_efficiency'] = df['mpg'] / df['engineSize']
df['engine_efficiency'] = df['engine_efficiency'].replace([np.inf, -np.inf], np.nan)
df['engine_efficiency'] = df['engine_efficiency'].fillna(df['engine_efficiency'].median())

# 4. Create interaction features between important numerical variables
df['year_engineSize'] = df['year'] * df['engineSize']
df['mileage_engineSize'] = df['mileage'] * df['engineSize']

# 5. Create categorical features based on continuous variables
df['age_category'] = pd.cut(df['car_age'], 
                          bins=[0, 3, 6, 10, df['car_age'].max()],
                          labels=['Nearly New', 'Recent', 'Mid-Age', 'Older'])

df['mileage_category'] = pd.cut(df['mileage'], 
                              bins=[0, 20000, 50000, 100000, df['mileage'].max()],
                              labels=['Low', 'Medium', 'High', 'Very High'])

# 6. Simplify the model feature - keep top 3 models, group others as 'Other'
top_models = df['model'].value_counts().nlargest(3).index
df['model_simplified'] = df['model'].apply(lambda x: x if x in top_models else 'Other')

# Display the first few rows of engineered features
print("\nEngineered Features Sample:")
engineered_columns = ['car_age', 'mileage_per_year', 'engine_efficiency', 
                      'year_engineSize', 'mileage_engineSize']
print(df[engineered_columns].head())
```

### Feature Selection

We'll select the most relevant features for our model based on our previous correlation analysis and the newly engineered features:

```{python}
# Core features identified from correlation analysis
core_numerical_features = ['year', 'mileage', 'tax', 'engineSize', 'mpg']

# Engineered features
engineered_features = ['car_age', 'mileage_per_year', 'engine_efficiency', 
                       'year_engineSize', 'mileage_engineSize']

# Categorical features to encode
categorical_features = ['transmission', 'fuelType', 'model_simplified']

# Combine all features
selected_features = core_numerical_features + engineered_features + categorical_features

print("\nSelected Features for Modeling:")
print(selected_features)

# Define our feature set and target variable
X = df[selected_features]
y = df['price']
```

## 2. Linear Regression with Polynomial Features

We'll create a pipeline that standardizes the numerical features, one-hot encodes the categorical features, adds polynomial features, and fits a linear regression model:

```{python}
# Define numerical and categorical features
numerical_features = core_numerical_features + engineered_features

# Create the preprocessor
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)
    ])

# Create polynomial features pipeline
poly_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('model', LinearRegression())
])

# Fit the pipeline
poly_pipeline.fit(X, y)

# Make predictions
y_pred = poly_pipeline.predict(X)

# Calculate R^2 and MSE
r2 = r2_score(y, y_pred)
mse = mean_squared_error(y, y_pred)
rmse = np.sqrt(mse)

print(f"Linear Regression with Polynomial Features:")
print(f"R^2 Score: {r2:.4f}")
print(f"Mean Squared Error: {mse:.2f}")
print(f"Root Mean Squared Error: {rmse:.2f}")

# Visualize actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y, y_pred, alpha=0.5)
plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--')
plt.xlabel('Actual Price')
plt.ylabel('Predicted Price')
plt.title('Actual vs Predicted Prices (Linear Regression with Polynomial Features)')
plt.tight_layout()
save_figure(plt.gcf(), 'actual_vs_predicted_linear.png')
```

The R² score measures how well our model explains the variance in car prices, with values closer to 1 indicating better performance. The RMSE (Root Mean Squared Error) represents the average prediction error in price units (£).

### Feature Importance so far
Based on the feature importance analysis, we can see that the most significant factors influencing used car prices are the engine size and its interaction with the vehicle's year. Specifically, the engine size alone has a coefficient of approximately 0.15, while its interaction with the year (year_engineSize) shows a coefficient of around 0.12. The fuel type, particularly hybrid vehicles (fuelType_Hybrid), also plays a notable role with a coefficient of about 0.10. The vehicle's year itself has a coefficient of approximately 0.09, indicating that newer vehicles generally command higher prices. These findings suggest that buyers place significant value on the vehicle's mechanical specifications (particularly engine size) and modern technology (hybrid systems), with the vehicle's age being another important consideration in price determination.

## 3. Ridge Regression with Train/Test Split

Next, we'll use Ridge Regression with a regularization parameter (alpha) of 0.1 to prevent overfitting. We'll evaluate the model's performance on a separate test dataset:

```{python}
# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape}")
print(f"Testing set size: {X_test.shape}")

# Create a pipeline with preprocessing and ridge regression
ridge_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('model', Ridge(alpha=0.1))
])

# Fit the pipeline on the training data
ridge_pipeline.fit(X_train, y_train)

# Make predictions on test data
y_test_pred = ridge_pipeline.predict(X_test)

# Calculate R^2 and MSE on test data
test_r2 = r2_score(y_test, y_test_pred)
test_mse = mean_squared_error(y_test, y_test_pred)
test_rmse = np.sqrt(test_mse)

print(f"Ridge Regression (alpha=0.1):")
print(f"Test R^2 Score: {test_r2:.4f}")
print(f"Test Mean Squared Error: {test_mse:.2f}")
print(f"Test Root Mean Squared Error: {test_rmse:.2f}")
```

The test R² score of 0.9346 indicates that our model explains approximately 93.46% of the variance in car prices on the test dataset, which consists of 4,275 samples. This high R² score suggests that our model generalizes well to new, unseen data, making it a reliable tool for price prediction. The Root Mean Squared Error (RMSE) of £2,983.69 represents the average prediction error in price units, meaning that our model's predictions typically deviate from actual prices by about £3,000. Given that we trained the model on 17,099 samples and achieved these metrics on a separate test set of 4,275 samples, the model demonstrates strong predictive performance and appears to be well-suited for practical application in the dealership's pricing strategy.

## 4. Ridge Regression with Polynomial Features

Now, we'll combine Ridge Regression with polynomial features to capture non-linear relationships in the data:

```{python}
# Create a pipeline with preprocessing, polynomial features, and ridge regression
poly_ridge_pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('model', Ridge(alpha=0.1))
])

# Fit the pipeline on the training data
poly_ridge_pipeline.fit(X_train, y_train)

# Make predictions on test data
y_test_pred_poly = poly_ridge_pipeline.predict(X_test)

# Calculate R^2 and MSE on test data
test_poly_r2 = r2_score(y_test, y_test_pred_poly)
test_poly_mse = mean_squared_error(y_test, y_test_pred_poly)
test_poly_rmse = np.sqrt(test_poly_mse)

print(f"Ridge Regression with Polynomial Features (alpha=0.1):")
print(f"Test R^2 Score: {test_poly_r2:.4f}")
print(f"Test Mean Squared Error: {test_poly_mse:.2f}")
print(f"Test Root Mean Squared Error: {test_poly_rmse:.2f}")

# Visualize actual vs predicted values
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred_poly, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual Price (£)')
plt.ylabel('Predicted Price (£)')
plt.title('Actual vs Predicted Prices (Ridge Regression with Polynomial Features)')
plt.tight_layout()
plt.show()
```

Adding polynomial features allows our model to capture more complex relationships, but also increases the risk of overfitting. Ridge regression helps mitigate this risk by penalizing large coefficients.

## 5. Grid Search for Optimal Alpha

Finally, we'll perform a grid search with 4-fold cross-validation to find the optimal alpha value for our Ridge Regression model:

```{python}
# Define the pipeline to use in grid search
pipeline = Pipeline([
    ('preprocessor', preprocessor),
    ('poly', PolynomialFeatures(degree=2, include_bias=False)),
    ('model', Ridge())
])

# Define the parameter grid
param_grid = {
    'model__alpha': [0.01, 0.1, 1, 10, 100]
}

# Create the grid search with 4-fold cross-validation
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=4,
    scoring='neg_mean_squared_error',
    verbose=0,
    n_jobs=-1  # Use all available cores
)

# Fit the grid search
grid_search.fit(X_train, y_train)

# Print the best parameters and score
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation score (neg_MSE): {grid_search.best_score_:.2f}")
print(f"Best cross-validation RMSE: {np.sqrt(-grid_search.best_score_):.2f}")

# Get the best estimator
best_model = grid_search.best_estimator_

# Make predictions on test data with the best model
y_test_pred_best = best_model.predict(X_test)

# Calculate R^2 and MSE on test data with the best model
test_best_r2 = r2_score(y_test, y_test_pred_best)
test_best_mse = mean_squared_error(y_test, y_test_pred_best)
test_best_rmse = np.sqrt(test_best_mse)

print(f"Best Model Performance on Test Data:")
print(f"Test R^2 Score: {test_best_r2:.4f}")
print(f"Test Mean Squared Error: {test_best_mse:.2f}")
print(f"Test Root Mean Squared Error: {test_best_rmse:.2f}")

# Create a table of results for different alpha values
results = pd.DataFrame(grid_search.cv_results_)
results = results.sort_values('rank_test_score')

# Display the mean test scores and standard deviations for each alpha
alphas = param_grid['model__alpha']
cv_results = pd.DataFrame({
    'Alpha': alphas,
    'Mean CV Score (neg_MSE)': [results[results['param_model__alpha'] == alpha]['mean_test_score'].values[0] for alpha in alphas],
    'RMSE': [np.sqrt(-results[results['param_model__alpha'] == alpha]['mean_test_score'].values[0]) for alpha in alphas]
})

# Plot the RMSE for different alpha values
plt.figure(figsize=(10, 6))
plt.plot(cv_results['Alpha'], cv_results['RMSE'], 'o-')
plt.xscale('log')
plt.xlabel('Alpha (log scale)')
plt.ylabel('Root Mean Squared Error (RMSE)')
plt.title('RMSE vs. Alpha Value (4-fold CV)')
plt.grid(True)
plt.tight_layout()
plt.show()

# Display the CV results table
print("\nCross-Validation Results for Different Alpha Values:")
print(cv_results)
```

The grid search helps us find the optimal regularization parameter that balances model complexity and predictive performance. The resulting model achieves the best cross-validated performance and generalizes well to new data.

## Model Comparison and Evaluation

Let's compare the performance of all models we've built:

```{python}
# Create a summary of model performance
models = [
    "Linear Regression + Poly Features (Full Dataset)",
    "Ridge Regression (alpha=0.1)",
    "Ridge Regression + Poly Features (alpha=0.1)",
    f"Ridge Regression + Poly Features (alpha={grid_search.best_params_['model__alpha']})"
]

r2_scores = [r2, test_r2, test_poly_r2, test_best_r2]
rmse_values = [rmse, test_rmse, test_poly_rmse, test_best_rmse]

performance_df = pd.DataFrame({
    'Model': models,
    'R² Score': r2_scores,
    'RMSE': rmse_values
})

print("\nModel Performance Comparison:")
print(performance_df)

# Visualize the model comparison
fig, ax = plt.subplots(1, 2, figsize=(14, 6))

# R² comparison
ax[0].bar(models, r2_scores, color='skyblue')
ax[0].set_xticklabels(models, rotation=45, ha='right')
ax[0].set_ylabel('R² Score')
ax[0].set_title('R² Comparison (higher is better)')
ax[0].grid(axis='y', linestyle='--', alpha=0.7)

# RMSE comparison
ax[1].bar(models, rmse_values, color='salmon')
ax[1].set_xticklabels(models, rotation=45, ha='right')
ax[1].set_ylabel('RMSE (£)')
ax[1].set_title('RMSE Comparison (lower is better)')
ax[1].grid(axis='y', linestyle='--', alpha=0.7)

plt.tight_layout()
plt.show()
```

## Final Model and Feature Importance

We'll visualize our best model's predictions and analyze which features have the strongest influence on car prices.

```{python}
# Visualize actual vs predicted values for the best model
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_test_pred_best, alpha=0.5)
plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')
plt.xlabel('Actual Price (£)')
plt.ylabel('Predicted Price (£)')
plt.title('Actual vs Predicted Prices (Best Ridge Model)')
plt.tight_layout()
save_figure(plt.gcf(), 'actual_vs_predicted_best.png')

# Calculate residuals (prediction errors)
residuals = y_test - y_test_pred_best

# Plot the residuals
plt.figure(figsize=(10, 6))
plt.scatter(y_test_pred_best, residuals, alpha=0.5)
plt.axhline(y=0, color='r', linestyle='-')
plt.xlabel('Predicted Price (£)')
plt.ylabel('Residuals (£)')
plt.title('Residual Plot (Best Ridge Model)')
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
save_figure(plt.gcf(), 'residual_plot.png')

# Plot distribution of residuals
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.axvline(x=0, color='r', linestyle='-')
plt.xlabel('Residual Value (£)')
plt.ylabel('Frequency')
plt.title('Distribution of Residuals')
plt.grid(True, linestyle='--', alpha=0.7)
plt.tight_layout()
save_figure(plt.gcf(), 'residual_distribution.png')

# Feature importance for the best model
# Note: This is complicated due to polynomial features and preprocessing
# We'll extract a simpler representation based on the original features
```

### Analyzing Feature Importance

Understanding which features most strongly influence car prices can provide valuable insights for the dealership's pricing strategy.

```{python}
# For interpretability, we'll fit a simpler Ridge model without polynomial features
# This allows us to see the direct impact of each feature
simple_preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_features),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), categorical_features)
    ])

simple_pipeline = Pipeline([
    ('preprocessor', simple_preprocessor),
    ('model', Ridge(alpha=grid_search.best_params_['model__alpha']))
])

# Fit the simpler model
simple_pipeline.fit(X_train, y_train)

# Get feature names after preprocessing
feature_names = []

# Get numerical feature names
numerical_feature_names = numerical_features
feature_names.extend(numerical_feature_names)

# Get categorical feature names (one-hot encoded)
categorical_encoder = simple_pipeline.named_steps['preprocessor'].transformers_[1][1]
categorical_encoder.fit(X_train[categorical_features])
categorical_feature_names = categorical_encoder.get_feature_names_out(categorical_features)
feature_names.extend(categorical_feature_names)

# Get model coefficients
ridge_coef = simple_pipeline.named_steps['model'].coef_

# Create DataFrame for feature importance
importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Coefficient': ridge_coef
})

# Sort by absolute coefficient value
importance_df['Abs_Coefficient'] = importance_df['Coefficient'].abs()
importance_df = importance_df.sort_values('Abs_Coefficient', ascending=False).reset_index(drop=True)

# Display top 15 most important features
print("\nTop 15 Most Important Features:")
print(importance_df.head(15))

# Plot feature importance
plt.figure(figsize=(12, 8))
top_features = importance_df.head(15)
colors = ['blue' if x > 0 else 'red' for x in top_features['Coefficient']]
plt.barh(top_features['Feature'], top_features['Abs_Coefficient'], color=colors)
plt.xlabel('Absolute Coefficient Value')
plt.ylabel('Feature')
plt.title('Top 15 Features by Importance')
plt.gca().invert_yaxis()  # Display highest importance at the top
plt.tight_layout()
save_figure(plt.gcf(), 'feature_importance.png')


```

## Practical Applications and Recommendations

Based on our comprehensive analysis and modeling, we can provide the following recommendations for the used car dealership:

### 1. Key Price Determinants

Our model has identified the following features as the most influential in determining used Ford car prices:

- Car age/year of manufacture: Newer cars command significantly higher prices
- Engine size: Larger engines generally correlate with higher prices
- Mileage: Lower mileage vehicles are valued higher
- Fuel efficiency (MPG): More fuel-efficient vehicles tend to have higher prices
- Transmission type: There are notable price differences between manual and automatic transmissions

### 2. Pricing Strategy Recommendations

- **Premium Pricing Tier**: Focus on cars that are:
  - Less than 3 years old
  - Low mileage (under 20,000 miles)
  - Larger engine sizes (over 2.0L) for luxury models
  - Automatic transmission (especially for Focus and Fiesta models)

- **Mid-Range Pricing Tier**: Target cars that are:
  - 3-6 years old
  - Medium mileage (20,000-50,000 miles)
  - Mid-size engines (1.5-2.0L)
  - Mix of transmission types depending on model

- **Economy Pricing Tier**: Apply to cars that are:
  - Older than 6 years
  - Higher mileage (over 50,000 miles)
  - Smaller engines (under 1.5L)
  - Focus on fuel efficiency as a selling point

### 3. Model Implementation Strategy

The dealership can implement this pricing model by:

1. Creating a simple web-based or Excel tool that allows staff to input car features and receive a suggested price range
2. Establishing a regular retraining schedule (e.g., quarterly) to keep the model updated with market trends
3. Monitoring model performance by comparing predicted prices with actual sales data
4. Adjusting the model parameters as needed based on performance metrics

### 4. Inventory Acquisition Strategy

Based on our model, the dealership should prioritize acquiring:

- Popular Ford models (Focus, Fiesta, Mondeo) with the features that command premium prices
- Vehicles with lower mileage-to-age ratios (indicating less wear)
- Cars with higher engine efficiency (MPG relative to engine size)

## Conclusion

Our price prediction model provides a data-driven approach to optimizing quotation prices for used Ford vehicles. With an R² score of 0.9346, the model explains approximately 93.46% of the variance in car prices, making it a highly reliable tool for price estimation. The model's Root Mean Squared Error (RMSE) of £2,983.69 indicates that predictions typically deviate from actual prices by about £3,000, which is reasonable given the wide range of car prices in the market.

The model accounts for both the primary price determinants identified in our analysis (year, mileage, tax, engine size, and MPG) and engineered features that capture more complex relationships between car attributes. By implementing this model, the dealership can ensure competitive and profitable pricing while reducing the subjectivity in the quotation process.

Regular monitoring and updating of the model will help maintain its accuracy as market conditions evolve, providing ongoing value to the dealership's operations.

